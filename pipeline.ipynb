{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "In diesem File findet die ganze Verarbeitung der Daten statt. Die Einstellungen sind im unteren Codeblock zu finden.\n",
    "\n",
    "Hier gäbe es performancetechnisch noch einiges zu optimieren, jedoch liegt das ausserhalb des Scopes dieser Arbeit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing\n",
    "Vorbereitung der Packages und Einstellungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Settings\n",
    "path_tickets_export = \"Daten/tickets_export.xlsx\"\n",
    "stopwords_url = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt'\n",
    "num_top_words_per_tickets = 10\n",
    "num_top_words_total = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Datenverarbeitung als Grundlage für die nachfolgenden Schritte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Erstellt</th>\n",
       "      <th>Aktualisiert</th>\n",
       "      <th>Gelöst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-31 12:59:24</td>\n",
       "      <td>2022-08-14 00:00:01</td>\n",
       "      <td>2022-07-31 16:37:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-31 10:22:23</td>\n",
       "      <td>2022-08-10 00:01:27</td>\n",
       "      <td>2022-08-02 06:59:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-31 05:25:30</td>\n",
       "      <td>2022-08-23 00:00:41</td>\n",
       "      <td>2022-08-15 15:34:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-07-30 22:59:57</td>\n",
       "      <td>2022-08-12 13:34:32</td>\n",
       "      <td>2022-08-08 09:04:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07-30 21:53:21</td>\n",
       "      <td>2022-08-10 00:01:27</td>\n",
       "      <td>2022-08-02 07:59:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63507</th>\n",
       "      <td>2022-12-01 07:06:13</td>\n",
       "      <td>2022-12-07 00:00:06</td>\n",
       "      <td>2022-12-01 08:12:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63508</th>\n",
       "      <td>2022-12-01 07:06:10</td>\n",
       "      <td>2022-12-01 07:06:10</td>\n",
       "      <td>2022-12-01 07:06:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63509</th>\n",
       "      <td>2022-12-01 07:05:35</td>\n",
       "      <td>2022-12-08 09:00:19</td>\n",
       "      <td>2022-12-01 08:13:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63510</th>\n",
       "      <td>2022-12-01 07:04:47</td>\n",
       "      <td>2022-12-08 10:00:15</td>\n",
       "      <td>2022-12-01 09:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63511</th>\n",
       "      <td>2022-12-01 09:00:14</td>\n",
       "      <td>2022-12-15 14:00:05</td>\n",
       "      <td>2022-12-08 13:58:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63512 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Erstellt         Aktualisiert               Gelöst\n",
       "0      2022-07-31 12:59:24  2022-08-14 00:00:01  2022-07-31 16:37:53\n",
       "1      2022-07-31 10:22:23  2022-08-10 00:01:27  2022-08-02 06:59:32\n",
       "2      2022-07-31 05:25:30  2022-08-23 00:00:41  2022-08-15 15:34:45\n",
       "3      2022-07-30 22:59:57  2022-08-12 13:34:32  2022-08-08 09:04:23\n",
       "4      2022-07-30 21:53:21  2022-08-10 00:01:27  2022-08-02 07:59:14\n",
       "...                    ...                  ...                  ...\n",
       "63507  2022-12-01 07:06:13  2022-12-07 00:00:06  2022-12-01 08:12:54\n",
       "63508  2022-12-01 07:06:10  2022-12-01 07:06:10  2022-12-01 07:06:10\n",
       "63509  2022-12-01 07:05:35  2022-12-08 09:00:19  2022-12-01 08:13:12\n",
       "63510  2022-12-01 07:04:47  2022-12-08 10:00:15  2022-12-01 09:45:00\n",
       "63511  2022-12-01 09:00:14  2022-12-15 14:00:05  2022-12-08 13:58:25\n",
       "\n",
       "[63512 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(path_tickets_export)\n",
    "\n",
    "# Process Dates\n",
    "data['Erstellt'] = pd.to_datetime(data['Erstellt'], unit=\"D\", origin=pd.Timestamp('1899-12-30')).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "data['Aktualisiert'] = pd.to_datetime(data['Aktualisiert'], unit=\"D\", origin=pd.Timestamp('1899-12-30')).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "data['Gelöst'] = pd.to_datetime(data['Gelöst'], unit=\"D\", origin=pd.Timestamp('1899-12-30')).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "data[[\"Erstellt\", \"Aktualisiert\", \"Gelöst\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Betreff</th>\n",
       "      <th>Beschreibung</th>\n",
       "      <th>Auflösungshinweise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tribuna funktioniert nicht kommt ständig eine ...</td>\n",
       "      <td>siehe oben                category   software ...</td>\n",
       "      <td>file system des datenbank systems ist vollgela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>passwort zurücksetzen für saxer vera  gsi-zshk...</td>\n",
       "      <td>guten tag  ich habe untenstehende mail erhalte...</td>\n",
       "      <td>guten morgen frau saxer  besten dank für die i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>merci d'ouvrir les accès intranet ajv-rgmo à t...</td>\n",
       "      <td>les accès intranet ajv-rgmo à tamara tedde  mm...</td>\n",
       "      <td>dieses ticket gibt es in doppelter ausführung ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kein zugriff mehr auf den hauptposteingang und...</td>\n",
       "      <td>category   software subcategor...</td>\n",
       "      <td>der fehler konnte behoben werden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>personne détenue en doublon</td>\n",
       "      <td>la personne détenue n'a été entrée qu'une seul...</td>\n",
       "      <td>guten tag  die id person 201691 ist nur einmal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Betreff   \n",
       "0  tribuna funktioniert nicht kommt ständig eine ...  \\\n",
       "1  passwort zurücksetzen für saxer vera  gsi-zshk...   \n",
       "2  merci d'ouvrir les accès intranet ajv-rgmo à t...   \n",
       "3  kein zugriff mehr auf den hauptposteingang und...   \n",
       "4                        personne détenue en doublon   \n",
       "\n",
       "                                        Beschreibung   \n",
       "0  siehe oben                category   software ...  \\\n",
       "1  guten tag  ich habe untenstehende mail erhalte...   \n",
       "2  les accès intranet ajv-rgmo à tamara tedde  mm...   \n",
       "3                  category   software subcategor...   \n",
       "4  la personne détenue n'a été entrée qu'une seul...   \n",
       "\n",
       "                                  Auflösungshinweise  \n",
       "0  file system des datenbank systems ist vollgela...  \n",
       "1  guten morgen frau saxer  besten dank für die i...  \n",
       "2  dieses ticket gibt es in doppelter ausführung ...  \n",
       "3                  der fehler konnte behoben werden   \n",
       "4  guten tag  die id person 201691 ist nur einmal...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process Text\n",
    "data['Betreff'] = data['Betreff'].astype(str).copy()\n",
    "data['Beschreibung'] = data['Beschreibung'].astype(str).copy()\n",
    "data['Auflösungshinweise'] = data['Auflösungshinweise'].astype(str).copy()\n",
    "\n",
    "def process_captions(data, column):\n",
    "    data[column] = data[column].apply(lambda x: x.replace('\\n', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.replace('--', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.replace(' - ', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.replace(':', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.replace(';', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.replace(',', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.replace('.', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.replace('!', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.replace('?', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.replace('(', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.replace(')', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.lower())\n",
    "    data[column] = data[column].apply(lambda x: x.replace('[^A-Za-z]', ''))\n",
    "    return data[column]\n",
    "\n",
    "data['Betreff'] = process_captions(data, 'Betreff').copy()\n",
    "data['Beschreibung'] = process_captions(data, 'Beschreibung').copy()\n",
    "data['Auflösungshinweise'] = process_captions(data, 'Auflösungshinweise').copy()\n",
    "\n",
    "data[[\"Betreff\", \"Beschreibung\", \"Auflösungshinweise\"]].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Berechnung der Keywords mittels TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63512it [16:40, 63.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Concat Free Text\n",
    "data[\"freetext\"] = data[\"Betreff\"] + \" \" + data[\"Beschreibung\"] + \" \" + data[\"Auflösungshinweise\"]\n",
    "\n",
    "# Fit TF-IDF with Stopwords\n",
    "stopwords = requests.get(stopwords_url).text.split(\"\\n\")\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords)\n",
    "tfidf.fit(data[\"freetext\"])\n",
    "\n",
    "# Convert vocabulary to DataFrame\n",
    "df_tfidf = pd.DataFrame.from_dict(tfidf.vocabulary_, orient=\"index\")\n",
    "df_tfidf.sort_values(by=0, ascending=False)\n",
    "df_tfidf = df_tfidf.reset_index()\n",
    "df_tfidf.columns = [\"word\", \"index\"]\n",
    "\n",
    "# Get Top Words\n",
    "top_words_i_list = []\n",
    "for i, row in tqdm(data.iterrows()):\n",
    "    top_words = sorted(\n",
    "        list(enumerate(tfidf.transform([row[\"freetext\"]]).toarray()[0])),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )[:num_top_words_per_tickets]\n",
    "\n",
    "    # create empty list to store top words \n",
    "    top_words_i = []\n",
    "    for word_index, word_score in top_words:\n",
    "        top_words_i.append(df_tfidf[df_tfidf[\"index\"] == word_index][\"word\"].values[0])\n",
    "    \n",
    "    # append top words to top_words_i_list\n",
    "    top_words_i_list.append(top_words_i)\n",
    "\n",
    "# Add top words to data\n",
    "data[\"top_words_each_ticket\"] = top_words_i_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to Excel\n",
      "Exported to Parquet\n"
     ]
    }
   ],
   "source": [
    "# Export to Excel and Parquet\n",
    "data.to_excel(\"Daten/tickets_export_topn_words.xlsx\", index=False)\n",
    "print(\"Exported to Excel\")\n",
    "data.to_parquet(\"Daten/tickets_export_topn_words.parquet\", index=False)\n",
    "print(\"Exported to Parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoding Top N Words\n",
    "OneHotEncodierung der wichtigsten Wörter im ganzen Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  7.07it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_parquet(\"Daten/tickets_export_topn_words.parquet\")\n",
    "\n",
    "# get all top words from all tickets\n",
    "important_words_df = pd.DataFrame(data[\"top_words_each_ticket\"].explode()).value_counts()\n",
    "\n",
    "# convert from multiindex to single index\n",
    "important_words_df = important_words_df.reset_index().set_index(\"top_words_each_ticket\")\n",
    "\n",
    "# filter words\n",
    "## remove words with numbers \n",
    "important_words_df = important_words_df[important_words_df.index.str.contains(r'[0-9]') == False]\n",
    "\n",
    "## if none remove\n",
    "important_words_df = important_words_df[important_words_df.index != \"none\"]\n",
    "important_words_df = important_words_df[important_words_df.index != \"nan\"]\n",
    "\n",
    "# get top num_top_words_total words\n",
    "important_words = important_words_df.head(num_top_words_total)\n",
    "\n",
    "# onehot encoding important words\n",
    "for word in tqdm(important_words.index):\n",
    "    data[word] = data[\"top_words_each_ticket\"].apply(lambda x: 1 if word in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to Excel\n",
      "Exported to Parquet\n"
     ]
    }
   ],
   "source": [
    "# Export to Excel and Parquet\n",
    "data.to_excel(\"Daten/tickets_export_topn_words_onehotencoded.xlsx\", index=False)\n",
    "print(\"Exported to Excel\")\n",
    "data.to_parquet(\"Daten/tickets_export_topn_words_onehotencoded.parquet\", index=False)\n",
    "print(\"Exported to Parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get coordinates \n",
    "Koordinaten zu den Orten hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 695/695 [05:56<00:00,  1.95it/s]\n",
      "100%|██████████| 120/120 [01:02<00:00,  1.92it/s]\n",
      "100%|██████████| 81/81 [00:41<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_parquet(\"Daten/tickets_export_topn_words_onehotencoded.parquet\")\n",
    "\n",
    "# function to get the coordinates of a location\n",
    "def get_coordinates(location):\n",
    "    url = f\"https://nominatim.openstreetmap.org/search?q={location}&format=json\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return (data[0]['lon'], data[0]['lat']) if len(data) > 0 else (None, None)\n",
    "\n",
    "# get all the unique locations and their coordinates\n",
    "unique_locations = data[\"Ort\"].unique()\n",
    "unique_coordinates = [get_coordinates(location) for location in tqdm(unique_locations)]\n",
    "\n",
    "# create a dataframe with the locations and coordinates\n",
    "streetmapping = pd.DataFrame({\"location\": unique_locations, \"coordinates\": unique_coordinates})\n",
    "\n",
    "# get streets with no coordinates\n",
    "missing_streets = streetmapping[streetmapping[\"coordinates\"] == (None, None)].copy()\n",
    "\n",
    "# replace \\n and other stuff with a comma\n",
    "missing_streets[\"location\"] = missing_streets[\"location\"].str.replace(\"\\n\", \", \").replace(\"\\r\", \", \").replace(\"\\t\", \", \").copy()\n",
    "\n",
    "# remove middlepart of the location\n",
    "# e.g. \"Bernstrasse 5, Postfach 207, 3312 Fraubrunnen\" -> \"Bernstrasse 5, 3312 Fraubrunnen\"\n",
    "missing_streets[\"location\"] = missing_streets[\"location\"].str.split(\", \").str[0] + \", \" + missing_streets[\"location\"].str.split(\", \").str[-1].copy()\n",
    "\n",
    "# get the coordinates for the missing streets\n",
    "missing_coordinates = [get_coordinates(location) for location in tqdm(missing_streets[\"location\"])]\n",
    "\n",
    "# add the coordinates to the dataframe\n",
    "missing_streets[\"coordinates\"] = missing_coordinates\n",
    "\n",
    "# add the missing coordinates to the streetmapping dataframe\n",
    "streetmapping.loc[streetmapping[\"coordinates\"] == (None, None), \"coordinates\"] = missing_streets[\"coordinates\"]\n",
    "\n",
    "# get the missing streets once again\n",
    "missing_streets = streetmapping[streetmapping[\"coordinates\"] == (None, None)].copy()\n",
    "\n",
    "# get last part of the location\n",
    "# e.g. \"Bernstrasse 5, Postfach 207, 3312 Fraubrunnen\" -> \"3312 Fraubrunnen\"\n",
    "missing_streets[\"location\"] = missing_streets[\"location\"].str.split(\", \").str[-1:].str.join(\", \").copy()\n",
    "# add \", Switzerland\" to the location\n",
    "missing_streets[\"location\"] = missing_streets[\"location\"] + \", Switzerland\"\n",
    "\n",
    "\n",
    "# get the coordinates for the missing streets\n",
    "missing_coordinates = [get_coordinates(location) for location in tqdm(missing_streets[\"location\"])]\n",
    "\n",
    "# add the coordinates to the dataframe\n",
    "missing_streets[\"coordinates\"] = missing_coordinates\n",
    "\n",
    "# add the missing coordinates to the streetmapping dataframe\n",
    "streetmapping.loc[streetmapping[\"coordinates\"] == (None, None), \"coordinates\"] = missing_streets[\"coordinates\"]\n",
    "\n",
    "# convert coordinates to lon and lat columns\n",
    "streetmapping[\"lon\"] = streetmapping[\"coordinates\"].str[0].astype(float)\n",
    "streetmapping[\"lat\"] = streetmapping[\"coordinates\"].str[1].astype(float)\n",
    "\n",
    "# remove the coordinates column\n",
    "streetmapping = streetmapping.drop(\"coordinates\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to Excel\n",
      "Exported to Parquet\n"
     ]
    }
   ],
   "source": [
    "# Export to Excel and Parquet\n",
    "streetmapping.to_excel(\"Daten/streetmapping.xlsx\", index=False)\n",
    "print(\"Exported to Excel\")\n",
    "streetmapping.to_parquet(\"Daten/streetmapping.parquet\", index=False)\n",
    "print(\"Exported to Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup data\n",
    "data = data.merge(streetmapping, left_on='Ort', right_on='location').copy()\n",
    "data = data.drop(columns=['location']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to Excel\n",
      "Exported to Parquet\n"
     ]
    }
   ],
   "source": [
    "# Export to Excel and Parquet\n",
    "data.to_excel(\"Daten/tickets_export_topn_words_onehotencoded_with_coordinates.xlsx\", index=False)\n",
    "print(\"Exported to Excel\")\n",
    "data.to_parquet(\"Daten/tickets_export_topn_words_onehotencoded_with_coordinates.parquet\", index=False)\n",
    "print(\"Exported to Parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
